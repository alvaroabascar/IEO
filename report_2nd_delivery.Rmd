---
output: html_document
---
# Re-analysis of the microarray data published by Murat, A. et al. (2008) "Stem Cell–Related “Self-Renewal” Signature and High Epidermal Growth Factor Receptor Expression Associated With Resistance to Concomitant Chemoradiotherapy in Glioblastoma"

### Álvaro Abella Bascarán (alvaro.abella01@estudiant.upf.edu)
### Eloi Casals (eloi.casals01@estudiant.upf.edu)
### Samuel Miravet Verde (samuel.miravet01@estudiant.upf.edu)

## Introduction

Glioblastoma multiforme is the most presented and aggressive brain tumor in humans, involving glial cells, with an incidence of 2–3 cases per 100,000 person life-years in Europe and North America ([Fonnet E. Bleeker, _et al_ (2012)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3337398/)).

Treatment can involve chemotherapy, radiation and surgery. Median survival with standard-of-care radiation and chemotherapy with the alkylating agent temozolomide is 15 months([Johnson, Derek R. _et al_ (2011)](http://link.springer.com/article/10.1007%2Fs11060-011-0749-4)) while the median survival without treatment is 4 and a half months. Regretfully glioblastomas are notorious for resistance to therapy, which has been attributed to DNA-repair proficiency, a multitude of deregulated molecular pathways, and, more recently, to the particular biologic behavior of tumor stem-like cells. Here, based on the reference work of [Murat, A. _et al_ (2008)](http://www.ncbi.nlm.nih.gov/pubmed/18565887), we aimed to identify the molecular profiles specific for treatment resistance to the current standard of care of concomitant chemoradiotherapy with temozolomide.

To achieve our goal, we take from the reference study a set of gene expression profiles of 80 glioblastomas of patients treated whithin clinical trials of concomitant and adjuvant temozolomide to radiotherapy (n=52) and patients treated with only radioterapy (n=28). In addition, 4 control patients were added to the study.


This document should be processed from R and you need to install the packages
[knitr](http://cran.r-project.org/web/packages/knitr/index.html) and
[markdown](http://cran.r-project.org/web/packages/markdown/index.html). Once
they are installed, you have to type the following instructions that generate
a HTML document that you can open with a web browser:

```
library(knitr)     ## required for "knitting" from Rmd to md
library(markdown)  ## required for processing from md to HTML
knit("projectTemplate.Rmd", "projectTemplate.md")  ## process Rmd to md
markdownToHTML("projectTemplate.md", "projectTemplate.html") ## process md to HTML
browseURL("projectTemplate.html") ## open the resulting HTML file from R
```

## Data
The microarray data used during the research of [Murat, A. _et al_ (2008)](http://www.ncbi.nlm.nih.gov/pubmed/18565887) can be found at the [Gene Expression Omnibus](http://www.ncbi.nlm.nih.gov/geo/), with the accession number [GSE7696](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE7696). In order to proceed with this analysis, the file [GSE7696_RAW.tar](http://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE7696&format=file) must be uncompressed under the directory 'data/raw/' and the file [GSE7696_series_matrix.txt.gz](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE7nnn/GSE7696/matrix/GSE7696_series_matrix.txt.gz) must be placed inside 'data/'.


## Subsampling

The original data contained 84 samples, of which 4 are controls, 28 correspond to patients treated with radiotherapy, and 52 to patients treated with both TMZ and radiotherapy. In order to have a more manageable analysis we decided to proceed, by now, with only 21 samples. The following script was used to do a subsampling keeping the original proportions among controls, TMZ/radiotherapy and radiotherapy samples.

We need GEOquery to retrieve the assay data and affy to handle data from Affymetrix 3'-biased Arrays.
```{r libraries, message=FALSE}
library(affy)
library(GEOquery)
```

This is a simple way to cache the full eset (all the assay data) in a simple object, to avoid
loading it from the .txt.gz the next time (a costly process).

```{r cache_data}
# Cache the data to avoid having to load it again next time from the .txt
if (file.exists('full_eset.rds')) {
  eset = readRDS('full_eset.rds')
} else {
  eset = getGEO(filename='data/GSE7696_series_matrix.txt.gz')
  saveRDS(eset, 'full_eset.rds')
}
```


We separate the sample names by treatment group, and take enough samples from each group to have a total of 21, keeping the original proportions.

```{r subsampling_step1}
# extract sample names
samples = colnames(eset)

# take only treated samples
treated = samples[eset$characteristics_ch1.5 != "treatment: NA"]

# turn sample names into filenames
subsample_filenames = paste(treated, '.cel.gz', sep='')

# read only the necessary the expression data
glioData = ReadAffy(celfile.path = "./data/raw", filenames=subsample_filenames)

nsamples = length(sampleNames(glioData))
sampleNames(glioData) = c(1:nsamples)
```

We also filter the original assay data to keep only the data corresponding to the samples we have choosen. We finally save both objects for posterior use. 
```{r subsampling_step3}
subsampled_eset = eset[, treated]

# save the subsampled data for posterior use

if (!file.exists('eset.rds')) {
  saveRDS(subsampled_eset, 'eset.rds')
}
if (!file.exists('glioData.rds')) {
  saveRDS(glioData, 'glioData.rds')
}

eset = subsampled_eset
```

## Quality assessment

```{r}
library(affyPLM)
```

### Chip inspection
We plot, in the first place, the chip images in order to detect artifacts and discard those samples corresponding to malfunctioning chips.
```{r chip_images, out.width="800px", dpi=400}
# Scanner plot analysis
par(mfrow = c(3, 7), mar = c(1, 1, 3, 1))
image(glioData)
```

From these images we cannot see any obvious artifact, and so far we can consider all of the samples as valid.

### Analysis of intensities
We can further assess the quality of the samples analysing the distribution of intensities from each chip. Chips with a distribution which differs significantly from the rest (in median or dispersion) should be taken with caution.

A first step is to check the distribution of raw intensities in logarithmic scale:
```{r intensity_boxplots, out.width="800px"}
# Raw intensity boxplot
par(mar = c(4, 5, 0, 1))
boxplot(glioData)
```

From the previous boxplots we see that every sample follows a similar distribution. Sample number "u" appears to have a slightly narrower and displaced distribution, but not enough to consider it wrong.

We can get some more information (eg. bimodalities) by means of a non-parametric density estimation of raw intensity values:
````{r density_plots, out.width="800px", dpi=300}
# Density plot
par(mar = c(3, 3, 4, 1))
plotDensity.AffyBatch(glioData, lwd=2, col=1:nsamples, lty=1:nsamples)
legend('topright', sampleNames(glioData), col=1:nsamples, lty=1:nsamples, lwd=2, inset=0.1)
```

We see that some of the samples are slightly shifted to the right, but still follow the same kind of distribution.

```{r}
badSamplesRawDist = ""
```

### Probe level modeling

We can assess the quality of Affymetrix chips using a linear model which relates the log intensities to the probe affinity effects, the array effects and the gaussian noise. Once we have the model we can represent the residuals (middle) and weights (right). 
````{r intensities, out.width="800px"}
Pset = fitPLM(glioData)
for (i in 1:nsamples) {
  par(mfrow=c(1, 2))
  image(Pset, type="resids", which=i) # PLM resids for sample A
  image(Pset, type="weights", which=i) # PLM weights for sample A
}
```

```{r}
badSamplesPLMResids = c("38", "27", "8")
```

###  Normalized Unscaled Standard Errors (NUSE)

By means of the NUSE we examine the median and interquartile range of all probesets in the chip, to obtain an overall view of chip expression quality.
````{r NUSE, out.width="800px"}
NUSE(Pset)
```

In this case we can take as valid those samples which don't deviate from the rest by more than a 5%. We can consider the following as deviating from the rest.

```{r}
badSamplesNUSE = c("18", "29", "31", "65", "74")
```

### Relative Log Expression (RLE)
Relative Log Expression Values are calculated for each of the probesets. In tis case we compare the expression on each array against the median expression value for the probeset across all of the arrays. The RLE summaries can be helpful to detect technical sources of variability that are large compared to biological variation.

````{r RLE, out.width="800px"}
# RLE
RLE(Pset)
```

All the samples display a similar median and interquartile range, with none of them exhibiting a very noticable deviation. 

Besides the graphical inspection, we can also take a look at the median and interquantile range for each sample.
````{r nuse_rle_stats}
nuseDiag = NUSE(Pset, type='stats')
rleDiag = RLE(Pset, type='stats')
nuseDiag
rleDiag
```

As in the case of the graphical inspection, we cannot see any sample which deviates significantly from the rest.

```{r}
badSamplesRLE = c()
```

## Normalization

In order to normalize the Affymetrix expression data we resort to the Robust Multi-array Average (RMA) algorithm. This method integrates the background correction, between-array normalization and summary.
```{r rma}
# RMA normalization (Robust Multi Array)
rmaEset = rma(glioData)
```

### MA plots

The MA plot can help us detect intensity dependent biases. In this case we compare (using a log ratio) the intensity of the red and green channels of the microarray (each representing a different condition). As we expect most of the genes to not be differentially expressed, most of the log ratios should be close to zero. Plotting the log ratio against the mean log intensity of both channels, we can detect dependences of the ratios on the fluorescence intensity.

In this case, we expect the red line to remain close to the blue one. A deviation from it is suggesting a systematic dependence of the ratios on the intensity, indicating poor quality of the expression data.
```{r MA, out.width="800px"}
for (i in c(1, 21, 41, 61)) {
  par(mfrow=c(5, 4), mar=c(.4, 4, .4, .4))
  MAplot(rmaEset[,i:(i+19)], plot.method='smoothScatter', cex=0.75, ref.title='')
}
```


In this case we don't see any case extremely obvious, but being strict we can consider as wrong the following set of samples:
```{r}
badSamplesMA = c("1", "39", "71")
```

We can finally display a table showing the number of diagnostics failed by each sample:

```{r}
qaDiag <- data.frame(RawDist = rep(FALSE, ncol(eset)), PLMresids = rep(FALSE, ncol(eset)), NUSE = rep(FALSE, ncol(eset)), RLE = rep(FALSE, ncol(eset)), MA = rep(FALSE, ncol(eset)), Failed = rep(0, ncol(eset)), row.names = sampleNames(glioData))

qaDiag[badSamplesRawDist, "RawDist"] <- TRUE
qaDiag[badSamplesPLMResids, "PLMresids"] <- TRUE
qaDiag[badSamplesNUSE, "NUSE"] <- TRUE
qaDiag[badSamplesRLE, "RLE"] <- TRUE
qaDiag[badSamplesMA, "MA"] <- TRUE

qaDiag$Failed <- rowSums(qaDiag)
qaDiag <- qaDiag[order(qaDiag$Failed, decreasing = TRUE), ]
head(qaDiag, 10)
```

In general we have achieved very good results as none of the samples fail more than 1 tests so we decide to continue the study with all the samples.

## Batch identification

The main goal of this step consists in finding if there exist any possible batch effect in our dataset, that is technical sources of variation that have been added to the samples during handling ([Leek, _et al_ (2010)](http://www.nature.com/nrg/journal/v11/n10/full/nrg2825.html#close)). It is important to detect them in order to ensure that the possible future differences between expressions do not come from a non-biological or scientific variables source.

We start the process loading the following libraries. 

```{r message=FALSE}
library(affy)
library(Biobase)
library(GEOquery)
library(affyPLM)
library(corpcor)
```

Between all the different features our dataset have, the only possible batch source is the experiment date of each sample so this will be our batch grouping variable. To consider it, we have to set a scanDate variable in the correct format:

```{r}
scanDate = protocolData(glioData)$ScanDate
scanDate = gsub(" .*", "", scanDate)
scanDate = as.Date(scanDate, "%m/%d/%y")
```

Once defined that variable, we are already able to group together the samples obtained in the same date:

```{r}
minscan = min(scanDate)
days = scanDate- minscan

sort(days)
```

We see 7 different batch groups. In order to ease the process of batch identification, we define 4 new groups based on weeks (1: 0, 2, 3 / 2: 7, 10 / 3: 28 / 4: 192). With those different week groups we can discretize the weeks numeric variable into desired intervals to finally obtain our surrogate batch indicator variable. In addition, the length of the variable _weeks_ allows us to define a vector of colors that will be used in the posterior plots.

```{r}
# Assign new groups by weeks:
batch_weeks <- cut(as.numeric(days), c(-1, 6, 20, 50, 200))
batch_weeks <- as.numeric(batch_weeks)
batch_weeks
```

The differential outcome for our dataset is the recurrence as the aim of the study is to define a set of genes expressed differentially between resistant cancer patients and non-resistant. To consider it, we extract the variable from the _eset_ and transform it into a character string.

```{r}
disease_status = eset$characteristics_ch1.1
disease_status = gsub('.*: ', '', disease_status)
disease_status = as.character(disease_status)
```

The division of our outcome respect the batch groups can be resumed with the _table_ function:

```{r}
table(data.frame(Outcome = disease_status, Batch = batch_weeks))
```

We see the batch variable across samples is clearly not balanced.

### Hierarchical Clustering

A nice way to see how effectively batch is confounding the outcome of interest consists of doing a hierarchical clustering of the samples indicating the batch to which each of them belongs to. We should start by calculating the distance between every pair of samples using a non-parametric association measure such as [Spearman correlation](http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient):

```{r}
d = as.dist(1 - cor(exprs(eset), method="spearman"))
```

Using _hclust_ we perform the hierarchical clustering which can be easily understood, showing it in a [dendrogram](http://en.wikipedia.org/wiki/Dendrogram). We are trying to identify the batch effect (the batch to which each sample belongs is indicated by the colour of the label), and a differential clustering separating samples by survival time (indicated in the sample label).

```{r}
# Hierarchical analysis:

sampleClustering = hclust(d)

# Function to generate the dendrogram:

batch = batch_weeks
sampleDendrogram <- as.dendrogram(sampleClustering, hang = 0.1)
names(batch) <- sampleNames(eset)
names(disease_status) <- sampleNames(eset)
sampleDendrogram <- dendrapply(sampleDendrogram, function(x, batch, labels) {
    ## for every node in the dendrogram if it is a leaf node
    if (is.leaf(x)) {
        attr(x, "nodePar") <- list(lab.col = as.vector(batch[attr(x, "label")]))  ## color by batch
        attr(x, "label") <- as.vector(labels[attr(x, "label")])  ## label by disease_status
    }
    x
}, batch, disease_status) 

# And the required command to plot it:

plot(sampleDendrogram, main = "Hierarchical clustering of samples")
legend("topright", paste("Batch", sort(unique(batch))), fill = sort(unique(batch)))
```

In this dendrogram we can see that our samples do not cluster by batch, indicating that apparently in this case we don't have batch effect. We would like to identify a separation by recurrence, but it is clear that the different recurrences are mixed. We don't see, so far, a relationship among different recurrence and expression.

### Multidimensional Scaling

Another way to verify whether batch is the main source of variation is [multidimensional scaling](http://en.wikipedia.org/wiki/Multidimensional_scaling) analysis. To perform it, we have to run the function _cmdscale_. The resulting _cmd_ object have to be a matrix of dimension (n,2) where n=number of samples.

```{r}
cmd = cmdscale(as.dist(1 - cor(exprs(eset), method = "spearman")))

dim(cmd)
head(cmd)

# Ploting the result
plot(cmd, type = "n")
text(cmd, disease_status, col = batch, cex = 0.9)
legend("topleft", paste("Batch", unique(batch)), fill = unique(batch), inset = 0.01)
```

All the recurrence status are mixed well and we cannot detect any color group in the distribution meaning that there is no bacth effect that could confound a possible differential classification of the outcome. 

### Quantifying Confounding

The last test we are going to perform is the quantification of confounding between batch and outcome by means of a [Principal Component Analysis](http://en.wikipedia.org/wiki/Principal_component_analysis) test. It works rebuilding the data set in new variables (Principal Components) in terms of linear combinations, such that they capture most of the variance of the data. The purpose of applying PCA in this case is to compare the principal components that account for the largest variability of the data, to known variables such as a batch indicator.

We start the process computing the [single value decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) of the data:

```{r}
s = fast.svd(t(scale(t(exprs(eset)), center=TRUE, scale=TRUE)))
```

This value allows us to plot the fraction of variance explained by each principal component as follows:

```{r}
plot(s$d^2/sum(s$d^2), type="b", lwd=2, las=1, xlab="Principal Component", ylab="Proportion of variance")
```

Using the function _cumsum_ we can evaluate the amount variance accumulated with each principal component:

```{r}
head(cumsum(s$d^2/sum(s$d^2)), 12)
```

In this case we explain more than 50% of variance considering 11 principal components. 

Finally, in order to get an estimate the amount of variability driven by batch, we should inspect the correlations between our batch indicator variable and the right-singular vectors in the component _v_ of _s_:

```{r}
par(mfrow=c(3, 4))
for (i in 1:12) {
  boxplot(split(s$v[, i], batch), main=sprintf("PC%d %.0f%%", i, 100 * s$d[i]^2/sum(s$d^2)))
}
```

We can see the distribution, for each of the batch subgroups, of each of the twelve principal components. We can appreciate that the batch number 2 has a wider distribution across components 1 and 2, while batch number 4 exhibits more variance in components 4 and 5. The results are, however, hard to interpret due to the low (and different) number of samples in each of the subgroups and we should not infer a batch effect from this analysis.

### Surrogate Variable Analysis

With the SVA we try to identify sources of heterogeneity (eg. non-biological variability due to batch effects). With this analysis we should obtain an estimation of the surrogate variables and their values, which we can later use to adjust for undesirable effects.

```{r}
library(sva)
# We need the outcome variable in binary format, with a threshold of 20 months of survival
st_bin <- ifelse(disease_status != "GBM", 1, 0)

# Define the full and null models
mod = model.matrix(~st_bin, data=pData(eset))
head(mod)

mod0 <- model.matrix(~1, data = pData(eset))

# surrogate variables calling
sv <- sva(exprs(eset), mod, mod0)

# Plot the correlations
par(mfrow = c(3, 3))
for (i in 1:sv$n.sv) boxplot(sv$sv[, i] ~ batch, main = sprintf("SV %d", i), xlab = "Batch")
```

The boxplots show almost no correlation of the batch indicator variable with every of the estimated surrogate variables.

The _sva_ package also provides a function to quickly perform F-tests for detecting genes significantly changing between the full and null models. This enables a quick overview of the impact of adjusting for the estimated heterogeneity:

```{r}
# Number of genes changing
pValues <- f.pvalue(exprs(eset), mod, mod0)
sum(p.adjust(pValues, method = "BH") < 0.05)

dim(eset)

# Changes after adjustment
modSv <- cbind(mod, sv$sv)
mod0Sv <- cbind(mod0, sv$sv)
pValuesSv <- f.pvalue(exprs(eset), modSv, mod0Sv)
sum(p.adjust(pValuesSv, method = "BH") < 0.05)
```

We see more than 4000 genes with a significant change between the full and null model. 

After the analysis conducted so far, we can assert that there is no batch effect in our samples. However, we have also seen that the samples corresponding to different recurrence have clustered in a rather heterogeneous way, which might indicate a lack of relationship among gene expression and survival when we take into account all the groups at the same time.

## Differential expression

With the aim to identify changes in gene expression associated to a specific condition, we perform a gene differential expression analysis. The condition in this analysis is the recurrence status in patients.

```{r "DE", message=FALSE}
library(Biobase)
library(genefilter)
library(limma)
library(sva)
library('hgu133plus2.db')
annotation(eset)<-'hgu133plus2.db'
```

First we setup the input data in a format suitable for the analysis.

```{r}
eset$characteristics_ch1.1 = factor(eset$characteristics_ch1.1) # disease status: re-recurrent GBM
eset$characteristics_ch1.2 = factor(eset$characteristics_ch1.2) # patient id
eset$characteristics_ch1.3 = factor(eset$characteristics_ch1.3) # age: dd.d
eset$characteristics_ch1.4 = factor(eset$characteristics_ch1.4) # gender: MALE/FEMALE
eset$characteristics_ch1.5 = factor(eset$characteristics_ch1.5) # treatment: radiotherapy or TMZ/radiotherapy
eset$characteristics_ch1.6 = factor(eset$characteristics_ch1.6) # survival status: 0 or 1
eset$characteristics_ch1.7 = factor(eset$characteristics_ch1.7) # survival time in months: 25.13
eset$characteristics_ch1.8 = factor(eset$characteristics_ch1.8) # mgmt status: M or U
eset$mgmt = eset$characteristics_ch1.8
eset$treatment = eset$characteristics_ch1.5

#eset$disease_status = ifelse(eset$characteristics_ch1.1 == "recurrent GBM", 1, 0)
eset$disease_status = grepl("recurrent",eset$characteristics_ch1.1)

# Transforming some variables to dichotomic
survival_threshold = 20
eset$survival_time_dichotomic = as.double(gsub('.*: ', '', eset$characteristics_ch1.7)) > survival_threshold
eset$survival_time_dichotomic = factor(eset$survival_time_dichotomic)

age_threshold = 50
eset$aged = as.double(gsub('.*: ', '', eset$characteristics_ch1.3)) > age_threshold
eset$aged = factor(eset$aged)
```

### Fold-change
 
To evaluate the expression levels we calculate the fold-change expression for each gene, comparing the expression levels between the surviving and non-surviving groups.
 
```{r}
zeroExp <- rowMeans(exprs(eset[, eset$disease_status]))
oneExp <- rowMeans(exprs(eset[, eset$disease_status != TRUE]))
par(mfrow = c(1, 2))
plot(zeroExp, oneExp, xlab = "Zero", ylab = "One", pch = ".", cex = 4, las = 1)
plot((oneExp + zeroExp)/2, oneExp - zeroExp, pch = ".", cex = 4, las = 1)
```

We can see that, in general, all the genes cluster together. We have certain points that deviate from the bulk, but not enough to imply an obvious differential expression. However, we can proceed with further analysis to verify this.

### Removing less variable genes

A first solution to avoid multiple testing problems consists in removing genes that are uninteresting biologically-wise. 

We could remove genes that show little variation following the  process that eliminates the 30% of genes with less variability in the following way:

```{r}
# IQRs <- esApply(eset, 1, IQR)
# plot.ecdf(IQRs, xlab = "IQR", main = "Empirical CDF of IQR values")
# abline(v = quantile(IQRs, prob = 0.3), col = "red", lwd = 2)
# 
# maskHighVariability <- IQRs > quantile(IQRs, prob = 0.3)
# length(maskHighVariability)
# 
# sum(maskHighVariability)
# 
# eset_filtered <- eset[maskHighVariability, ]
# 
# dim(eset_filtered)
```

The best way to reduce the number of analysed probe sets is by _nsFilter_. This utility function filters out features exhibiting little variation, or a consistently low signal, across samples, as well as features with insufficient annotation.

```{r}
filtered <- nsFilter(eset, require.entrez=TRUE,
         require.GOBP=FALSE, require.GOCC=FALSE,
         require.GOMF=FALSE, require.CytoBand=FALSE,
         remove.dupEntrez=TRUE, var.func=IQR,
         var.cutoff=0.5, var.filter=TRUE,
         filterByQuantile=TRUE, feature.exclude="^AFFX")
eset_filtered <- filtered$eset
```

With this, we have generated a new _eset_ dataset containing less probe sets: `r length(featureNames(eset_filtered))`, compared with the initial set of `r length(featureNames(eset))`.

### Moderated t-test using limma

To prevent problems associated to limited replication, we use the empirical Bayes method from the Bioconductor limma package to calculate a moderated t-test that takes into account the typical standard deviation of the genes.

We project our data to a linear model and calculate the moderated t-statistics.

```{r}
design <- model.matrix(~disease_status, data = eset_filtered)
fit <- lmFit(eset_filtered, design)
fit <- eBayes(fit)
res <- decideTests(fit, p.value = 0.1)
summary(res)
```

We can see from the summary result that there are no significantly expressed genes. We can also check the results from the distribution of p-values for all the genes and the volcano plot.

```{r}
tt <- topTable(fit, coef = 2, n = Inf)
par(mfrow = c(1, 2), mar = c(4, 5, 2, 2))
hist(tt$P.Value, xlab = "Raw P-values", main = "")
hist(tt$P.Value, xlab = "Raw P-values", breaks = 1000, main = "")
```

This distribution appears to be slightly negatively skewed (more density of p-values near 1). Adjusting for covariates we will be able to reduce this effect and obtain a more uniform distribution.

```{r out.width="600px"}
par(mfrow=c(1,1))
plot(tt$logFC, -log10(tt$P.Value), pch=".", cex=4, col=grey(0.75), cex.axis=1.2, las=1, cex.lab=1.5, xlab=expression(paste(log[2], " Fold change")), ylab=expression(paste(-log[10], " P-value")))
if (sum(tt$adj.P.Val < 0.1) > 0) {
  abline(h=-log10(max(tt[tt$adj.P.Val < 0.1, "P.Value"])), col=grey(0.5), lty=2)
}
points(tt[tt$adj.P.Val < 0.1, "logFC"], -log10(tt[tt$adj.P.Val < 0.1, "P.Value"]), pch=".", cex=4, col="red")
```

We see in red in the volcano plot those genes apparently differentially expressed. 

### Adjusting for covariates

We can adjust for the contribution of covariates. As proposed in the paper, we adjust for MGMT methylation status and for age>50.

```{r}
eset_filtered$mgmtM = eset_filtered$mgmt == "mgmt status: M"

# Surrogate Variables
# adjusted for age (> 50 years) and MGMT methylation status
mod <- model.matrix(~disease_status + mgmtM + aged, data = eset_filtered)
mod0 <- model.matrix(~ mgmtM + aged, data = eset_filtered)

svaobj <- sva(exprs(eset_filtered), mod, mod0)

modSVs <- cbind(mod, svaobj$sv)

fit <- lmFit(eset_filtered, modSVs)
fit <- eBayes(fit)
ttadj <- topTable(fit, coef = 2, n = Inf)

par(mfrow = c(1, 2), mar = c(4, 5, 2, 2))
hist(ttadj$P.Value, xlab = "Raw P-values", main = "")
hist(ttadj$P.Value, xlab = "Raw P-values", breaks = 1000, main = "")
```

Now we can see that the distribution is much closer to uniform. We can do again a volcano plot to check if the significances have changed.

```{r out.width="600px"}
plot(ttadj$logFC, -log10(ttadj$P.Value), pch=".", cex=4, col=grey(0.75), cex.axis=1.2, las=1, cex.lab=1.5, xlab=expression(paste(log[2], " Fold change")), ylab=expression(paste(-log[10], " P-value")))
if (sum(ttadj$adj.P.Val < 0.1) > 0) {
  abline(h=-log10(max(ttadj[ttadj$adj.P.Val < 0.1, "P.Value"])), col=grey(0.5), lty=2)
}
points(ttadj[ttadj$adj.P.Val < 0.1, "logFC"], -log10(ttadj[ttadj$adj.P.Val < 0.1, "P.Value"]), pch=".", cex=4, col="red")
```

We see that the number of genes differentially expressed have been increased after the correction. 

## First Conclusions

During this analysis we have explored the data provided by Murat, A. et al. (2008). We have performed a quality assessment, ensuring that the samples have enough quality to proceed with the analysis. In addition, we have tried to identify sources of batch effect, concluding that these samples are not affected by this type of technical bias. Finally, we have made a first attempt to detect differentially expressed genes on two groups of samples: those corresponding to patients with a recurrent glioblastoma. 

The analysis of differential expression has revealed some genes differentially expressed that we will study in detail in further steps. 

## Funtional Enrichment Analysis

Given a list of differentially expressed (DE) genes, we may know the function and role of some of those genes within the molecular process under study. This may already shed light on what is being investigated. However, even if we knew what every DE gene is doing, we would need to frame their activity within the pathways in which they are participating to make a hypothesis about why are they changing.

A closer model to the underlying biology is the one in which pathways, or gene sets, are DE. We may want then to do either the DE analysis directly at pathway level, or indirectly by assessing over-representation (enrichment) of our DE genes in every pathway of interest.

Therefore, our goal is to generate _gene sets_ following the steps:
1) Search for DE genes.
2) For every gene set, verify whether these DE genes belong to the gene set in a proportion that exceeds any expectation of finding that number of genes in that gene set by chance alone.

### Gene Ontology analysis

In order to do this we performa a _Gene Ontology Analysis_. We start searching for DE genes at 10% FDR between recurrent and non-recurrent samples from the results obtained using Limma in the previous step:

```{r}
library(hgu133plus2.db)

DEgenes <- rownames(ttadj[ttadj$adj.P.Val < 0.1, , drop = FALSE])
length(DEgenes)

# Select gene ID in database
DEgenes_indb <- select(hgu133plus2.db, DEgenes, "ENTREZID", "PROBEID")

universeIDs <- select(hgu133plus2.db, featureNames(eset_filtered), "ENTREZID", "PROBEID")
```

We see we have `r length(DEgenes)` differentially expressed. In order to define the gene ontology terms we use the _GOStats_ package. 

But we have to take into account one thing, a problem in a GO analysis is the hierarchy of GO terms and their overlap render highly dependent tests. If parent and a child GO term contain the same genes and both are significant, the child node is more relevant because is more specific. Compute the significance of a GO term conditional on the significance of its children. Proceed bottom-up removing all genes in a significant GO term from its parents. This is regarded as a conditional test that we have to apply using:

```{r}
library(GOstats)

# Begin building a parameter object as follows
params <- new("GOHyperGParams", geneIds = DEgenes_indb$ENTREZID, universeGeneIds = universeIDs$ENTREZID, annotation = "hgu133plus2.db", ontology = "BP", pvalueCutoff = 0.1, testDirection = "over")

conditional(params) <- TRUE  #To consider the conditional test

# Run the functional enrichment analysis.
hgOverCond <- hyperGTest(params)
hgOverCond

# And store and visualize the results.
htmlReport(hgOverCond, file = "gocondtests.html")
# browseURL("gocondtests.html")
```

# TODO : Alguna forma de añadir ese html como links y explicar mas o menos que se ve!

The resulting object from a call to hyperGTest() belongs to the class of objects GOHyperGResult. We can find out what methods are available to explore the results in detail. A few of the more useful ones are:

```{r}
head(summary(hgOverCond), n = 3)
head(geneCounts(hgOverCond)) 
head(universeCounts(hgOverCond))
head(pvalues(hgOverCond))
```

We inspect our results:

```{r}
goresults <- summary(hgOverCond)
head(goresults)
```

### Filter of GO terms
GO terms involving a few genes (e.g., < 5) in their size (m) and in their enrichment by DE genes are likely to be less reliable than those that involve many genes.

In order to try to spot the more reliable GO terms we can filter the previous results by a minimum value on the Count and Size columns and order them by the OddsRatio column:

```{r}
goresults <- goresults[goresults$Size >= 5 & goresults$Count >= 5, ]
goresults <- goresults[order(goresults$OddsRatio, decreasing = TRUE), ]
goresults
```

We can extract the genes that enrich each GO term and paste it to the result as follows:

```{r}
geneIDs <- geneIdsByCategory(hgOverCond)[goresults$GOBPID]
geneSYMs <- sapply(geneIDs, function(id) select(hgu133plus2.db, columns = "SYMBOL", key = id, keytype = "ENTREZID")$SYMBOL)
geneSYMs <- sapply(geneSYMs, paste, collapse = ", ")
goresults <- cbind(goresults, Genes = geneSYMs)
rownames(goresults) <- 1:nrow(goresults)
```

We can generate an HTML page from a data.frame object using the xtable package:

```{r}
library(xtable)
xtab <- xtable(goresults, align = "l|c|r|r|r|r|r|p{3cm}|p{3cm}|")
print(xtab, file = "goresults.html", type = "html")
```

#TODO : de nuevo, tratar de linkear este html y poner un poc que tienen los genes que aparecen

## Using GO enrichment in the study of Over/Under-expressed genes

Once we have defined the gene ontology terms related to the differentially expressed genes we may be interested in knowing over and underexpressed genes in the two conditions (recurrent and non-recurrent).

To do this, we separate the initial eset_filtered in two subsets, one for recurrent patients and non-recurrent:

```{r}
eset_recurrent = eset_filtered[, eset_filtered$disease_status]
eset_norecurrent = eset_filtered[, !eset_filtered$disease_status]
```

From this two expression sets we need to extract the mean expression values for all the samples for each gene differentially expressed. We achieve this iterating over the two groups of expressions and generating two vectors of genes:

```{r}
# Initial list of DE genes and vector definition
DEgenes <- rownames(ttadj[ttadj$adj.P.Val < 0.1, , drop = FALSE])
recurrent_overexpressed = c()
recurrent_underexpressed = c()

# Loop to compare mean expressions for those DE genes.
for (gene in DEgenes) {
  diff = mean(exprs(eset_recurrent)[gene,]) - mean(exprs(eset_norecurrent)[gene,])
  if (diff >= 0) {
    recurrent_overexpressed = c(recurrent_overexpressed, gene)
  } else {
    recurrent_underexpressed = c(recurrent_underexpressed, gene)
  }
}
```

### Overexpressed genes in recurrent state:

The next step consists on repeating the previous process of GO enrichment but this  time using only the subsets of DE genes with more expression in recurrent status.

```{r}
# Select gene ID in database
DEgenes_indb <- select(hgu133plus2.db, recurrent_overexpressed, "ENTREZID", "PROBEID")
universeIDs <- select(hgu133plus2.db, featureNames(eset_filtered), "ENTREZID", "PROBEID")

# Build a parameter object
params <- new("GOHyperGParams", geneIds = DEgenes_indb$ENTREZID, universeGeneIds = universeIDs$ENTREZID, annotation = "hgu133plus2.db", ontology = "BP", pvalueCutoff = 0.1, testDirection = "over")

conditional(params) <- TRUE  #To consider the conditional test

# Run the functional enrichment analysis.
hgOverCond <- hyperGTest(params)
hgOverCond

# And store the results.
htmlReport(hgOverCond, file = "go_overexpr_condtests.html")
# browseURL("gocondtests.html")
```

# TODO : Alguna forma de añadir ese html como links y explicar mas o menos que se

```{r}
goresults <- summary(hgOverCond)

# Filtering results with less than 5 of size:
goresults <- goresults[goresults$Size >= 5 & goresults$Count >= 5, ]
goresults <- goresults[order(goresults$OddsRatio, decreasing = TRUE), ]
goresults
```

We can extract the genes that enrich each GO term and are overexpressed in recurrent patients and paste it to the result as follows:

```{r}
geneIDs <- geneIdsByCategory(hgOverCond)[goresults$GOBPID]
geneSYMs <- sapply(geneIDs, function(id) select(hgu133plus2.db, columns = "SYMBOL", key = id, keytype = "ENTREZID")$SYMBOL)
geneSYMs <- sapply(geneSYMs, paste, collapse = ", ")
goresults <- cbind(goresults, Genes = geneSYMs)
rownames(goresults) <- 1:nrow(goresults)

# We can generate an HTML page from a data.frame object using the xtable package:

xtab <- xtable(goresults, align = "l|c|r|r|r|r|r|p{3cm}|p{3cm}|")
print(xtab, file = "go_overexpr_results.html", type = "html")
```

###Underexpressed genes in recurrent state:

Finally, we do the same but using the underexpressed gene set:

```{r}
# Select gene ID in database
DEgenes_indb <- select(hgu133plus2.db, recurrent_underexpressed, "ENTREZID", "PROBEID")

universeIDs <- select(hgu133plus2.db, featureNames(eset_filtered), "ENTREZID", "PROBEID")

# Build a parameter object as follows
params <- new("GOHyperGParams", geneIds = DEgenes_indb$ENTREZID, universeGeneIds = universeIDs$ENTREZID, annotation = "hgu133plus2.db", ontology = "BP", pvalueCutoff = 0.1, testDirection = "over")

conditional(params) <- TRUE  #To consider the conditional test

# Run the functional enrichment analysis.
hgOverCond <- hyperGTest(params)
hgOverCond

# And store and visualize the results.
htmlReport(hgOverCond, file = "go_underexpr_condtests.html")
# browseURL("gocondtests.html")
```

# TODO : Alguna forma de añadir ese html como links y explicar mas o menos que se ve!

```{r}
goresults <- summary(hgOverCond)

# Filter results:
goresults <- goresults[goresults$Size >= 5 & goresults$Count >= 5, ]
goresults <- goresults[order(goresults$OddsRatio, decreasing = TRUE), ]
goresults
```

We can extract the genes that enrich each GO term underexpressed in recurrent patients and paste it to the result as follows:

```{r}
geneIDs <- geneIdsByCategory(hgOverCond)[goresults$GOBPID]
geneSYMs <- sapply(geneIDs, function(id) select(hgu133plus2.db, columns = "SYMBOL", key = id, keytype = "ENTREZID")$SYMBOL)
geneSYMs <- sapply(geneSYMs, paste, collapse = ", ")
goresults <- cbind(goresults, Genes = geneSYMs)
rownames(goresults) <- 1:nrow(goresults)

# Generate the html file:
xtab <- xtable(goresults, align = "l|c|r|r|r|r|r|p{3cm}|p{3cm}|")
print(xtab, file = "go_underexpr_results.html", type = "html")
```

## Gene Set Enrichment Analysis

We can evaluate the differentially expressed GO terms for biological processes by using Gene Set Enrichment Analysis. The gene set collection used for the analysis is obtained from the Molecular Signatures Database [http://www.broadinstitute.org/gsea/msigdb/download_file.jsp?filePath=/resources/msigdb/5.0/c5.bp.v5.0.orig.gmt]

```{r}
library(GSEABase)
library(GSVAdata)

eset_GSEA <- eset_filtered
data(c2BroadSets)

# restrict this analysis to pathways from KEGG, REACTOME and BIOCARTA
c2BroadSets <- c2BroadSets[c(grep("^KEGG", names(c2BroadSets)),
               grep("^REACTOME", names(c2BroadSets)), grep("^BIOCARTA", names(c2BroadSets)))]

gsc <- c2BroadSets
gsc <- mapIdentifiers(gsc, AnnoOrEntrezIdentifier(annotation(eset_GSEA)))
```
We create the incidence matrix of the expression set and the gene set collection.

```{r}
Im <- incidence(gsc)
dim(Im)
Im[1:2, 10:20]
```

We need to discard genes that are not part of the data, as well as genes that are not present in the annotated gene sets.
```{r}
Im <- Im[, colnames(Im) %in% featureNames(eset_GSEA)]
dim(Im)

eset_GSEA <- eset_GSEA[colnames(Im), ]
eset_GSEA
```

For the simple GSEA analysis, we perform a classical DE evaluation without calling any gene DE
```{r}
# Surrogate Variables
# adjusted for age (> 50 years) and MGMT methylation status
mod <- model.matrix(~disease_status + mgmtM + aged, data = eset_GSEA)
mod0 <- model.matrix(~ mgmtM + aged, data = eset_GSEA)

svaobj <- sva(exprs(eset_GSEA), mod, mod0)

modSVs <- cbind(mod, svaobj$sv)

fit <- lmFit(eset_GSEA, modSVs)
fit <- eBayes(fit)
ttadj <- topTable(fit, coef = 2, n = Inf)
ttadj[1:10, c("Gene.Symbol", "logFC", "AveExpr", "t", "P.Value", "adj.P.Val", "B")]
```

We can verify the normality of the t-statistics on a qqplot. 

```{r}
qq <- qqnorm(ttadj$t)
qqline(ttadj$t)
```

We can evaluate the Z-scores to detect differential expressions within genesets, using the standardized sample mean of the geneset t-statistics. To add some robustness to the calculation of the Z-scores, we limit the evaluation to genesets having 5 or more genes. We store all moderated t-statistics for the genes forming the incidence matrix of gene sets and calculate its Z-score statistic.

```{r}
Im <- Im[rowSums(Im) > 5, ]
dim(Im)

tGSgenes <- ttadj[match(colnames(Im), rownames(ttadj)), "t"]
length(tGSgenes)

zS <- sqrt(rowSums(Im)) * (as.vector(Im %*% tGSgenes)/rowSums(Im))
length(zS)
head(zS)

qqnorm(zS)
qqline(zS)
```

The largest Z-scores correspond to the following gene sets
```{r}
rnkGS <- sort(abs(zS), decreasing = TRUE)
head(rnkGS, 50)
```

For select some of the significant GO terms and compare the mean expression values for the two phenotypes.

```{r}
plotGS <- function(eset, gs, pheno, ...) {
  l <- levels(as.factor(pData(eset)[, pheno]))
  idxSamples1 <- pData(eset)[, pheno] == l[1]
  idxSamples2 <- pData(eset)[, pheno] == l[2]
  exps1 <- rowMeans(exprs(eset[gs, idxSamples1]))
  exps2 <- rowMeans(exprs(eset[gs, idxSamples2]))
  rng <- range(c(exps1, exps2))
  plot(exps1, exps2, pch = 21, col = "black", bg = "black", xlim = rng, ylim = rng, xlab = l[1], ylab = l[2], ...)
  abline(a = 0, b = 1, lwd = 2, col = "red")
  }

genesGS1 <- colnames(Im)[which(Im["REACTOME_CELL_SURFACE_INTERACTIONS_AT_THE_VASCULAR_WALL", ] == 1)]
genesGS2 <- colnames(Im)[which(Im["REACTOME_P53_INDEPENDENT_DNA_DAMAGE_RESPONSE", ] == 1)]
genesGS3 <- colnames(Im)[which(Im["REACTOME_SIGNALING_IN_IMMUNE_SYSTEM", ] == 1)]
genesGS4 <- colnames(Im)[which(Im["REACTOME_TIGHT_JUNCTION_INTERACTIONS", ] == 1)]
genesGS5 <- colnames(Im)[which(Im["REACTOME_HEMOSTASIS", ] == 1)]
genesGS6 <- colnames(Im)[which(Im["REACTOME_SLC_MEDIATED_TRANSMEMBRANE_TRANSPORT", ] == 1)]

par(mfrow = c(2, 3), mar = c(4, 5, 3, 4))
plotGS(eset_GSEA, genesGS1, "disease_status", main = "REACTOME_CELL_SURFACE_INTERACTIONS_AT_THE_VASCULAR_WALL", cex.lab = 2, las = 1)
plotGS(eset_GSEA, genesGS2, "disease_status", main = "REACTOME_P53_INDEPENDENT_DNA_DAMAGE_RESPONSE", cex.lab = 2, las = 1)
plotGS(eset_GSEA, genesGS3, "disease_status", main = "REACTOME_SIGNALING_IN_IMMUNE_SYSTEM", cex.lab = 2, las = 1)
plotGS(eset_GSEA, genesGS4, "disease_status", main = "REACTOME_TIGHT_JUNCTION_INTERACTIONS", cex.lab = 2, las = 1)
plotGS(eset_GSEA, genesGS5, "disease_status", main = "REACTOME_HEMOSTASIS", cex.lab = 2, las = 1)
plotGS(eset_GSEA, genesGS6, "disease_status", main = "REACTOME_SLC_MEDIATED_TRANSMEMBRANE_TRANSPORT", cex.lab = 2, las = 1)
```

To calculate the adjusted p-values associated to each GO term, by and select those with FDR < 10%.

```{r}
pv <- pmin(pnorm(zS), 1 - pnorm(zS))
sum(pv < 0.1)
pvadj <- p.adjust(pv, method = "fdr")
DEgs <- names(pvadj)[which(pvadj < 0.1)]
length(DEgs)
head(DEgs, n = 3)
```
We also take into account the overlap between gene sets, to build a ranking of pairs of gene sets

```{r}
library(GSVA)
gsov <- computeGeneSetsOverlap(gsc[DEgs], eset_GSEA)
trimask <- upper.tri(gsov)
rnkOv <- data.frame(gs1 = row(gsov)[trimask], gs2 = col(gsov)[trimask], ov = gsov[trimask])
rnkOv <- rnkOv[order(rnkOv$ov, decreasing = TRUE), ]
rnkOv$gs1 <- rownames(gsov)[rnkOv$gs1]
rnkOv$gs2 <- rownames(gsov)[rnkOv$gs2]
sum(rnkOv$ov == 1) ## how many pairs of gene sets are identical?
sum(rnkOv$ov < 0.1) ## how many pairs of gene sets share less than 5% of the genes?
```


##Session information

```{r sessionInfo}
sessionInfo()
```


