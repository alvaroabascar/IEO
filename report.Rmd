# Re-analysis of the microarray data published by Murat, A. et al. (2008) "Stem Cell–Related “Self-Renewal” Signature and High Epidermal Growth Factor Receptor Expression Associated With Resistance to Concomitant Chemoradiotherapy in Glioblastoma"

### Álvaro Abella Bascarán (alvaro.abella01@estudiant.upf.edu)
### Eloi Casals (eloi.casals01@estudiant.upf.edu)
### Samuel Miravet Verde (samuel.miravet01@estudiant.upf.edu)

## Introduction

Glioblastoma multiforme is the most presented and aggressive brain tumor in humans, involving glial cells, with an incidence of 2–3 cases per 100,000 person life-years in Europe and North America ([Fonnet E. Bleeker, _et al_ (2012)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3337398/)).

Treatment can involve chemotherapy, radiation and surgery. Median survival with standard-of-care radiation and chemotherapy with the alkylating agent temozolomide is 15 months([Johnson, Derek R. _et al_ (2011)](http://link.springer.com/article/10.1007%2Fs11060-011-0749-4)) while the median survival without treatment is 4 and a half months. Regretfully glioblastomas are notorious for resistance to therapy, which has been attributed to DNA-repair proficiency, a multitude of deregulated molecular pathways, and, more recently, to the particular biologic behavior of tumor stem-like cells. Here, based on the reference work of [Murat, A. _et al_ (2008)](http://www.ncbi.nlm.nih.gov/pubmed/18565887), we aimed to identify the molecular profiles specific for treatment resistance to the current standard of care of concomitant chemoradiotherapy with temozolomide.

To achieve our goal, we take from the reference study a set of gene expression profiles of 80 glioblastomas of patients treated whithin clinical trials of concomitant and adjuvant temozolomide to radiotherapy (n=52) and patients treated with only radioterapy (n=28). In addition, 4 control patients were added to the study.


This document should be processed from R and you need to install the packages
[knitr](http://cran.r-project.org/web/packages/knitr/index.html) and
[markdown](http://cran.r-project.org/web/packages/markdown/index.html). Once
they are installed, you have to type the following instructions that generate
a HTML document that you can open with a web browser:

```
library(knitr)     ## required for "knitting" from Rmd to md
library(markdown)  ## required for processing from md to HTML
knit("projectTemplate.Rmd", "projectTemplate.md")  ## process Rmd to md
markdownToHTML("projectTemplate.md", "projectTemplate.html") ## process md to HTML
browseURL("projectTemplate.html") ## open the resulting HTML file from R
```

## Data
The microarray data used during the research of [Murat, A. _et al_ (2008)](http://www.ncbi.nlm.nih.gov/pubmed/18565887) can be found at the [Gene Expression Omnibus](http://www.ncbi.nlm.nih.gov/geo/), with the accession number [GSE7696](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE7696). In order to proceed with this analysis, the file [GSE7696_RAW.tar](http://www.ncbi.nlm.nih.gov/geo/download/?acc=GSE7696&format=file) must be uncompressed under the directory 'data/raw/' and the file [GSE7696_series_matrix.txt.gz](ftp://ftp.ncbi.nlm.nih.gov/geo/series/GSE7nnn/GSE7696/matrix/GSE7696_series_matrix.txt.gz) must be placed inside 'data/'.


<!--
## Subsampling

The original data contained 84 samples, of which 4 are controls, 28 correspond to patients treated with radiotherapy, and 52 to patients treated with both TMZ and radiotherapy. In order to have a more manageable analysis we decided to proceed, by now, with only 21 samples. The following script was used to do a subsampling keeping the original proportions among controls, TMZ/radiotherapy and radiotherapy samples.

We need GEOquery to retrieve the assay data and affy to handle data from Affymetrix 3'-biased Arrays.
```{r libraries, message=FALSE}
library(affy)
library(GEOquery)
```

This is a simple way to cache the full eset (all the assay data) in a simple object, to avoid
loading it from the .txt.gz the next time (a costly process)

```{r cache_data}
# Cache the data to avoid having to load it again next time from the .txt
if (file.exists('full_eset.rds')) {
  eset = readRDS('full_eset.rds')
} else {
  eset = getGEO(filename='data/GSE7696_series_matrix.txt.gz')
  saveRDS(eset, 'full_eset.rds')
}
```


We separate the sample names by treatment group, and take enough samples from each group to have a total of 21, keeping the original proportions.

```{r subsampling_step1}
# extract sample names
samples = colnames(eset)

# separate the samples by treatment group
no_treated = samples[eset$characteristics_ch1.5 == "treatment: NA"]
tmz_rt = samples[eset$characteristics_ch1.5 == "treatment: TMZ/radiotherapy"]
rt = samples[eset$characteristics_ch1.5 == "treatment: radiotherapy"]

# take 21 samples, keeping the proportions of the original dataset
set.seed(123456)
no_treated = sample(no_treated, size=1)
tmz_rt = sample(tmz_rt, size=13)
rt = sample(rt, size=7)
```

Join the three arrays of sample names, and turn them into file names. Then read only the necessary CEL files. This allows us to retrieve the data much faster, as we are loading only the 21 required samples, and not the whole set of 84 samples.

```{r subsampling_step2}
subsample_names = c(no_treated, tmz_rt, rt)
# turn sample names into filenames
subsample_filenames = paste(subsample_names, '.cel.gz', sep='')

# read only the necessary the expression data
glioData = ReadAffy(celfile.path = "./data/raw", filenames=subsample_filenames)

# display used samples:
subsample_names
```

We also filter the original assay data to keep only the data corresponding to the samples we have choosen. We finally save both objects for posterior use. 
```{r subsampling_step3}
subsampled_eset = eset[, subsample_names]

# save the subsampled data for posterior use
saveRDS(subsampled_eset, 'eset.rds')
saveRDS(glioData, 'glioData.rds')

eset = subsampled_eset
```

## Quality assessment

```{r}
library(affyPLM)
```

#### Chip inspection
We plot, in the first place, the chip images in order to detect artifacts and discard those samples corresponding to malfunctioning chips.
```{r chip_images, out.width="800px", dpi=400}
sampleNames(glioData) = letters[1:21]
# Scanner plot analysis
par(mfrow = c(3, 7), mar = c(1, 1, 3, 1))
image(glioData)
```

From this images we cannot see any obvious artifact, and so far we can consider all of the samples as valid.

#### Analysis of intensities
We can further assess the quality of the samples analysing the distribution of intensities from each chip. Chips with a distribution which differs significantly from the rest (in median or dispersion) should be taken with caution.

A first step is to check the distribution of raw intensities in logarithmic scale:
```{r intensity_boxplots, out.width="800px"}
# Raw intensity boxplot
par(mar = c(4, 5, 0, 1))
boxplot(glioData)
```
From the previous boxplots we see that every sample follows a similar distribution. Sample number "u" appears to have a slightly narrower and displaced distribution, but not enough to consider it wrong.

We can get some more information (eg. bimodalities) by means of a non-parametric density estimation of raw intensity values:
```{r density_plots, out.width="800px", dpi=300}
# Density plot
par(mar = c(3, 3, 4, 1))
plotDensity.AffyBatch(glioData, lwd=2, col=1:21, lty=1:21)
legend('topright', LETTERS[1:21], col=1:21, lty=1:21, lwd=2, inset=0.1)
```
We see that all samples follow a similar distribution except in the case of "u", which is shifted to the right and has a higher peak.

From the two previous diagnostics, we can consider sample "u" as wrong in terms of the distribution of intensities:
```{r}
badSamplesRawDistriution = "u"
```

#### Probe level modeling.

We can assess the quality of Affymetrix chips using a linear model which relates the log intensities to the probe affinity effects, the array effects and the gaussian noise. Once we have the model we can represent the raw intensities (left), residuals (middle) and weights (right). 
```{r intensities, out.width="800px"}
Pset = fitPLM(glioData)
for (i in 1:21) {
  par(mfrow=c(1, 3))
  image(glioData[, i]) # raw intensities for sample A
  image(Pset, type="resids", which=i) # PLM resids for sample A
  image(Pset, type="weights", which=i) # PLM weights for sample A
}
```

From the previous figures we can see that sample "j" appears to have a more intense spot, maybe not enough to discard it if the rest of analysis seem to be alright.

```{r}
badSamplesPLMResids = "j"
```

####  Normalized Unscaled Standard Errors (NUSE)

By means of the NUSE we examine the median and interquartile range of all probesets in the chip, to obtain an overall view of chip expression quality.
```{r NUSE, out.width="800px"}
NUSE(Pset)
```

In this case we can take as valid those samples which don't deviate from the rest by more than a 5%. As "u" is above 1.05, we can consider it as deviating from the rest.

```{r}
badSamplesNUSE = "u"
```

#### Relative Log Expression (RLE)
Relative Log Expression Values are calculated for each of the probesets. In tis case we compare the expression on each array against the median expression value for the probeset across all of the arrays. The RLE summaries can be helpful to detect technical sources of variability that are large compared to biological variation.

```{r RLE, out.width="800px"}
# RLE
RLE(Pset)
```
All the samples display a similar median and interquartile range, with none of them exhibiting a very noticable deviation. 

Besides the graphical inspection, we can also take a look at the median and interquantile range for each sample.
```{r nuse_rle_stats}
nuseDiag = NUSE(Pset, type='stats')
rleDiag = RLE(Pset, type='stats')
nuseDiag
rleDiag
```
As in the case of the graphical inspection, we cannot see any sample which deviates significantly from the rest.

## Normalization

In order to normalize the Affymetrix expression data we resort to the Robust Multi-array Average (RMA) algorithm. This method integrates the background correction, between-array normalization and summary.
```{r rma}
# RMA normalization (Robust Multi Array)
rmaEset = rma(glioData)
```

#### MA plots

The MA plot can help us detect intensity dependent biases. In this case we compare (using a log ratio) the intensity of the red and green channels of the microarray (each representing a different condition). As we expect most of the genes to not be differentially expressed, most of the log ratios should be close to zero. Plotting the log ratio against the mean log intensity of both channels, we can detect dependences of the ratios on the fluorescence intensity.

In this case, we expect the red line to remain close to the blue one. A deviation from it is suggesting a systematic dependence of the ratios on the intensity, indicating poor quality of the expression data.
```{r MA, out.width="800px"}
par(mfrow=c(3, 7), mar=c(1, 1, 1, 1))
MAplot(rmaEset[-grep('^AFFX', featureNames(rmaEset))], plot.method='smoothScatter', cex=0.75, ref.title='')
```


In this case we don't see any case extremely obvious, but being strict we can consider as wrong the following set of samples:
```{r}
badSamplesMA = c("c", "d", "o")
```

We can finally display a table showing the number of diagnostics failed by each sample:

<!-- TODO: FUCKING TABLE -->
```{r}
qaDiag <- data.frame(RawDist = rep(FALSE, ncol(eset)), PLMresids = rep(FALSE, ncol(eset)), NUSE = rep(FALSE, ncol(eset)), RLE = rep(FALSE, ncol(eset)), MA = rep(FALSE, ncol(eset)), Failed = rep(0, ncol(eset)), row.names = sampleNames(glioData))

qaDiag[badSamplesRawDist, "RawDist"] <- TRUE
qaDiag[badSamplesPLMresids, "PLMresids"] <- TRUE
qaDiag[badSamplesNUSE, "NUSE"] <- TRUE
qaDiag[badSamplesRLE, "RLE"] <- TRUE
qaDiag[badSamplesMA, "MA"] <- TRUE

qaDiag$Failed <- rowSums(qaDiag)
qaDiag <- qaDiag[order(qaDiag$Failed, decreasing = TRUE), ]
qaDiag
```


## Batch identification

The main goal of this step consists in finding if there exist any possible batch effect in our dataset, that is technical sources of variation that have been added to the samples during handling ([Leek, _et al_ (2010)](http://www.nature.com/nrg/journal/v11/n10/full/nrg2825.html#close)). It is important to detect them in order to ensure that the possible future differences between expressions do not come from a non-biological or scientific variables source.

After loading the needed libraries, we start the process retrieving the expression set of study and giving a specific letter to each sample.

```{r message=FALSE}
library(affy)
library(Biobase)
library(GEOquery)
library(affyPLM)
library(corpcor)

# glioData = readRDS('glioData.rds')
# eset = readRDS('eset.rds')
# sampleNames(glioData) = letters[1:21]
```

Between all the different features our dataset have, the only possible batch source is the experiment date of each sample so this will be our batch grouping variable. To consider it, we have to set a scanDate variable in the correct format:

```{r}
scanDate = protocolData(glioData)$ScanDate
scanDate = gsub(" .*", "", scanDate)
scanDate = as.Date(scanDate, "%m/%d/%y")
```

Once defined that variable, we are already able to group together the samples obtained in the same date:

```{r}
minscan = min(scanDate)
days = scanDate- minscan

sort(days)
```

With those different day groups we can discretize the days numeric variable into desired intervals to finally obtain our surrogate batch indicator variable. In addition, the length of the variable _days_ allows us to define a vector of colors that will be used in the posterior plots.

```{r}
batch_days = data.frame(row.names=sort(unique(days)), batch=c(1:length(unique(days))))

ourcolors = rainbow(length(unique(days)))
```

The differential outcome for our dataset is the survival time as the aim of the study is to define a set of genes expressed differentially between resistant cancer patients and non-resistant. To consider it, we extract the variable from the _eset_ and transform it into a character string.

```{r}
survival_time = eset$characteristics_ch1.7
survival_time = gsub('.*:', '', survival_time)
survival_time = as.character(survival_time)
```

The division of our outcome respect the batch groups can be resumed with the _table_ function:

```{r}
table(data.frame(Outcome = survival_time, Batch = days))
```

#### Hierarchical Clustering

A nice way to see how effectively batch is confounding the outcome of interest consists of doing a hierarchical clustering of the samples indicating the batch to which each of them belongs to. We should start by calculating the distance between every pair of samples using a non-parametric association measure such as [Spearman correlation](http://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient):

```{r}
d = as.dist(1 - cor(exprs(eset), method="spearman"))
```

Using _hclust_ we perform the hierarchical clustering which can be easily understood showing it in a [dendrogram](http://en.wikipedia.org/wiki/Dendrogram).

```{r}
# Hierarchical analysis:

sampleClustering = hclust(d)

# Function to generate the dendrogram:

sampleDendrogram = as.dendrogram(sampleClustering, hang=0.5)
batch = days
names(batch) = sampleNames(eset)
names(survival_time) = sampleNames(eset)
sampleDendrogram = dendrapply(sampleDendrogram, function(x, batch, labels) {
    ## for every node in the dendrogram if it is a leaf node
    if (is.leaf(x)) {
      print(x)
        attr(x, "nodePar") = list(lab.col = as.vector(ourcolors[batch_days[as.character(batch[attr(x, "label")]),]]))
        attr(x, "label") = as.vector(labels[attr(x, "label")])  ## label by survival_time
    }
    x
}, batch, survival_time)

# And the required command to plot it:

plot(sampleDendrogram, main = "Hierarchical clustering of samples")
legend("topright", paste("Batch", sort(unique(batch))), fill = ourcolors)
```

In the dendrogram we are able to see how there does not exist any batch effect confounding our outcome as the different survival times are randomly distributed and the colors (representing the batch group) do not cluster in concrete branches. 

#### Multidimensional Scaling

Another way to verify whether batch is the main source of variation is [multidimensional scaling](http://en.wikipedia.org/wiki/Multidimensional_scaling) analysis. To perform it, we have to run the function _cmdscale_. The resulting _cmd_ object have to be a matrix of dimension (n,2) where n=number of samples.

```{r}
cmd = cmdscale(as.dist(1 - cor(exprs(eset), method = "spearman")))

dim(cmd)
head(cmd)

# Ploting the result
plot(cmd, type = "n")
text(cmd, survival_time, col = ourcolors[batch_days[as.character(batch),]], cex = 0.9)
legend("topleft", paste("Batch", unique(batch)), fill = ourcolors[batch_days[as.character(unique(batch)),]], inset = 0.01)
```

All the survival times are mixed well and we cannot detect any color group in the distribution meaning that there is no bacth effect that could confound a possible differential classification of the outcome. 


#### Quantifying Confounding

The last test we are going to perform is the quantification of confounding between batch and outcome by means of a [Principal Component Analysis](http://en.wikipedia.org/wiki/Principal_component_analysis) test. It works rebuilding the data set in new variables (Principal Components) in terms of linear combinations, such that they capture most of the variance of the data. The purpose of applying PCA in this case is to compare the principal components that account for the largest variability of the data, to known variables such as a batch indicator.

We start the process computing the [single value decomposition](http://en.wikipedia.org/wiki/Singular_value_decomposition) (SVD) of the data:

```{r}
s = fast.svd(t(scale(t(exprs(eset)), center=TRUE, scale=TRUE)))
```

This value allows us to plot the fraction of variance explained by each principal component as follows:

```{r}
plot(s$d^2/sum(s$d^2), type="b", lwd=2, las=1, xlab="Principal Component", ylab="Proportion of variance")
```

Using the function _cumsum_ we can evaluate the amount variance accumulated with each principal component:

```{r}
head(cumsum(s$d^2/sum(s$d^2)))
```

# TODO : in this case we need X components to sum more than 50% of the variance. 

Finally, in order to get an estimate the amount of variability driven by batch, we should inspect the correlations between our batch indicator variable and the right-singular vectors in the component _v_ of _s_:

# TODO, adjust the boxplots to the number of components considered before
```{r}
par(mfrow=c(2, 3))
for (i in 1:6) {
  boxplot(split(s$v[, i], batch), main=sprintf("PC%d %.0f%%", i, 100 * s$d[i]^2/sum(s$d^2)))
}
```

# TODO: Interpretacion de esto???

# TODO: ADD Surrogate Variables steps???



## Differential expression

With the aim to identify changes in gene expression associated to a specific condition, we perform a gene differential expression analysis. The condition in this analysis is the survivavility of the tumoral condition in patients treated with concomitant chemoradiotherapy(TMZ/RT->TMZ).


```{r "DE", message=FALSE}
library(Biobase)
library(genefilter)
library(limma)
library(sva)
library('hgu133plus2.db')
annotation(eset)<-'hgu133plus2.db'

```

First we setup the input data in a format suitable for the analysis.

```{r}
eset = eset[, eset$characteristics_ch1.6 != "survival status: NA"] # Remove control patients
eset$characteristics_ch1.1 = factor(eset$characteristics_ch1.1) # disease status: re-recurrent GBM
eset$characteristics_ch1.2 = factor(eset$characteristics_ch1.2) # patient id
eset$characteristics_ch1.3 = factor(eset$characteristics_ch1.3) # age: dd.d
eset$characteristics_ch1.4 = factor(eset$characteristics_ch1.4) # gender: MALE/FEMALE
eset$characteristics_ch1.5 = factor(eset$characteristics_ch1.5) # treatment: radiotherapy or TMZ/radiotherapy
eset$characteristics_ch1.6 = factor(eset$characteristics_ch1.6) # survival status: 0 or 1
eset$characteristics_ch1.7 = factor(eset$characteristics_ch1.7) # survival time in months: 25.13
eset$characteristics_ch1.8 = factor(eset$characteristics_ch1.8) # mgmt status: M or U
eset$mgmt = eset$characteristics_ch1.8
eset$treatment = eset$characteristics_ch1.5

eset$survival_status = as.double(gsub('.*: ', '', eset$characteristics_ch1.6)) == 1
eset$survival_status = factor(eset$survival_status)

survival_threshold = 20
eset$survival_time = as.double(gsub('.*: ', '', eset$characteristics_ch1.7)) > survival_threshold
eset$survival_time = factor(eset$survival_time)

age_threshold = 50
eset$aged = as.double(gsub('.*: ', '', eset$characteristics_ch1.3)) > age_threshold
eset$aged = factor(eset$aged)

```

#### Fold-change
 
To evaluate the expression levels we calculate the fold-change expression for each gene, comparing the expression levels between the surviving and non-surviving groups.
 
```{r}
zeroExp <- rowMeans(exprs(eset[, eset$survival_time]))
oneExp <- rowMeans(exprs(eset[, eset$survival_time != TRUE]))
par(mfrow = c(1, 2))
plot(zeroExp, oneExp, xlab = "Zero", ylab = "One", pch = ".", cex = 4, las = 1)
plot((oneExp + zeroExp)/2, oneExp - zeroExp, pch = ".", cex = 4, las = 1)
```

#### Significance

We need to assess the significance of the differentially expressed genes, applying a Benjamini-Hochberg multiple test correction.

```{r}
allTests <- rowttests(eset, eset$survival_time)
min(allTests$p.value)
```

We need to adjust for multiple testing.

```{r}
# FDR adjustment
padjFDR <- p.adjust(allTests$p.value, method = "BH")
sum(padjFDR < 0.05)
min(padjFDR)
```

From this test it is clear that there are too many genes, and the multiple test correction renders all the tests insignificant due to the large number of tests. A possible solution to this problem consists in removing genes that are uninteresting biologically-wise.

First, we remove genes that show little variation. Following the procedure of the paper, we remove the genes that have a standard deviation below 0.75.

```{r}
maskHighVariability <- apply(exprs(eset), 1, sd)>0.75
eset_filtered <- eset[maskHighVariability, ]
```

This reduces the dataset from `r nrow(eset)` genes down to `r nrow(eset_filtered)`. Although there is an increase in significance with the lower amount of tested genes, it is still not enough, as seen on the minimal corrected p-value obtained.

```{r}
fewerTests <- rowttests(eset_filtered, eset_filtered$survival_time)
padjFDR <- p.adjust(fewerTests$p.value, method = "BH")
sum(padjFDR < 0.05)
min(padjFDR)
```

To reduce even more the number of analysed probe sets we use the _nsFilter_ function. This utility function filters out  features exhibiting little variation, or a consistently low signal, across samples, as well as features with insufficient annotation.

```{r}
filtered <- nsFilter(eset_filtered, require.entrez=TRUE,
         require.GOBP=FALSE, require.GOCC=FALSE,
         require.GOMF=FALSE, require.CytoBand=FALSE,
         remove.dupEntrez=TRUE, var.func=IQR,
         var.cutoff=0.5, var.filter=TRUE,
         filterByQuantile=TRUE, feature.exclude="^AFFX")
eset_filtered <- filtered$eset
fewerTests <- rowttests(eset_filtered, eset_filtered$survival_time)
padjFDR <- p.adjust(fewerTests$p.value, method = "BH")
sum(padjFDR < 0.05)
min(padjFDR)
```

The dataset is reduced to `r nrow(eset_filtered)` genes. As seen in the result, this improves a little bit further the significance, with a minimum p-value = `r signif(min(padjFDR), digits=2)`, but still is not enough.


#### Moderated t-test using limma

To prevent problems associated to limited replication, we use the empirical Bayes method from the Bioconductor limma package to calculate a moderated t-test that takes into account the typical standard deviation of the genes.

We project our data to a linear model and calculate the moderated t-statistics.

```{r}
#########eset_filtered <- eset_filtered[, eset_filtered$treatment == "treatment: TMZ/radiotherapy"]

design <- model.matrix(~survival_time, data = eset_filtered)
fit <- lmFit(eset_filtered, design)
fit <- eBayes(fit)
res <- decideTests(fit, p.value = 0.1)
summary(res)
```

We can see from the summary result that there are no significantly expressed genes. We can also check the results from the distribution of p-values for all the genes and the volcano plot.

```{r}
tt <- topTable(fit, coef = 2, n = Inf)
par(mfrow = c(1, 2), mar = c(4, 5, 2, 2))
hist(tt$P.Value, xlab = "Raw P-values", main = "")
hist(tt$P.Value, xlab = "Raw P-values", breaks = 1000, main = "")

plot(tt$logFC, -log10(tt$P.Value), pch=".", cex=4, col=grey(0.75), cex.axis=1.2, las=1, cex.lab=1.5, xlab=expression(paste(log[2], " Fold change")), ylab=expression(paste(-log[10], " P-value")))
points(tt[tt$adj.P.Val < 0.1, "logFC"], -log10(tt[tt$adj.P.Val < 0.1, "P.Value"]), pch=".", cex=4, col="red")
# abline(h=-log10(max(tt[tt$adj.P.Val < 0.1, "P.Value"])), col=grey(0.5), lty=2)
```

#### Adjusting for covariates

We can adjust for the contribution of covariates. As proposed in the paper, we adjust for MGMT methylation status and for age>50.

```{r}
eset_filtered$mgmtM = eset_filtered$mgmt == "mgmt status: M"

# Surrogate Variables
# adjusted for age (> 50 years) and MGMT methylation status
mod <- model.matrix(~survival_status + mgmtM + aged, data = eset_filtered)
mod0 <- model.matrix(~ mgmtM + aged, data = eset_filtered)

svaobj <- sva(exprs(eset_filtered), mod, mod0)

modSVs <- cbind(mod, svaobj$sv)

fit <- lmFit(eset_filtered, modSVs)
fit <- eBayes(fit)
ttadj <- topTable(fit, coef = 2, n = Inf)

par(mfrow = c(1, 2), mar = c(4, 5, 2, 2))
hist(ttadj$P.Value, xlab = "Raw P-values", main = "")
hist(ttadj$P.Value, xlab = "Raw P-values", breaks = 1000, main = "")
```

## Conclusion

## Session information

```{r}
sessionInfo()
```
